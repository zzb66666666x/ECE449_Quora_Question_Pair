{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Features extracted to find the similarity between two given questions. Various types of features are extracted, they are as follows :\n",
    "* Various distance similarities\n",
    "* Word Level Features\n",
    "* Character Level Features\n",
    "* Tf-idf features\n",
    "* Leaky Features which were disclosed during competetion\n",
    "* Miscellaneous Features\n",
    "\n",
    "**Note : **This notebook is python 3 compatible.\n",
    "\n",
    "All the related information about the features can be found in this great [post](https://www.kaggle.com/c/quora-question-pairs/discussion/32819)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import argparse\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to create more features on the top of features generated by Abhishek on kaggle which can be downloaded from [here](https://drive.google.com/file/d/0B-2wHXRXhQPqUlpyOWJYU2lYUUE/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     18,
     21,
     25,
     37,
     47,
     50,
     60,
     63,
     78,
     81,
     91,
     95,
     101,
     120,
     138
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def word_match_share(row, stops=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    return len(set(row['question1']).intersection(set(row['question2'])))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    return len(set(row['question1']).union(row['question2']))\n",
    "\n",
    "def total_unq_words_stop(row, stops):\n",
    "    return len([x for x in set(row['question1']).union(row['question2']) if x not in stops])\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    l1 = len(row['question1'])*1.0 \n",
    "    l2 = len(row['question2'])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(row['question1'])) * 1.0\n",
    "    l2 = len(set(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique_stop(row, stops=None):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def wc_ratio_unique_stop(row, stops=None):\n",
    "    l1 = len([x for x in set(row['question1']) if x not in stops])*1.0 \n",
    "    l2 = len([x for x in set(row['question2']) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    if not row['question1'] or not row['question2']:\n",
    "        return np.nan\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_ratio(row):\n",
    "    l1 = len(''.join(row['question1'])) \n",
    "    l2 = len(''.join(row['question2']))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def char_diff_unique_stop(row, stops=None):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n",
    "\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row, stops=None, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_features(data, stops, weights):\n",
    "    X = pd.DataFrame()\n",
    "    f = functools.partial(word_match_share, stops=stops)\n",
    "    X['word_match'] = data.apply(f, axis=1, raw=True) #1\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share, weights=weights)\n",
    "    X['tfidf_wm'] = data.apply(f, axis=1, raw=True) #2\n",
    "\n",
    "    f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\n",
    "    X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\n",
    "\n",
    "    X['jaccard'] = data.apply(jaccard, axis=1, raw=True) #4\n",
    "    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True) #5\n",
    "    X['wc_ratio'] = data.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    X['wc_ratio_unique'] = data.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "\n",
    "    f = functools.partial(wc_diff_unique_stop, stops=stops)    \n",
    "    X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\n",
    "    f = functools.partial(wc_ratio_unique_stop, stops=stops)    \n",
    "    X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\n",
    "\n",
    "    X['same_start'] = data.apply(same_start_word, axis=1, raw=True) #11\n",
    "    X['char_diff'] = data.apply(char_diff, axis=1, raw=True) #12\n",
    "\n",
    "    f = functools.partial(char_diff_unique_stop, stops=stops) \n",
    "    X['char_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #13\n",
    "    \n",
    "    X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\n",
    "    X['total_unique_words'] = data.apply(total_unique_words, axis=1, raw=True)  #15\n",
    "\n",
    "    f = functools.partial(total_unq_words_stop, stops=stops)\n",
    "    X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\n",
    "    \n",
    "    X['char_ratio'] = data.apply(char_ratio, axis=1, raw=True) #17    \n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading features\n",
      "loading actual data\n",
      "making leaky features\n",
      "Building Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/ipykernel_launcher.py:118: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating\n"
     ]
    }
   ],
   "source": [
    "# processing\n",
    "print ('loading features')\n",
    "df_train = pd.read_csv('train_features.csv', encoding=\"ISO-8859-1\")\n",
    "X_train_ab = df_train.iloc[:, 2:-1]\n",
    "X_train_ab = X_train_ab.drop('euclidean_distance', axis=1)\n",
    "X_train_ab = X_train_ab.drop('jaccard_distance', axis=1)\n",
    "\n",
    "print ('loading actual data')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train = df_train.fillna(' ')\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "ques = pd.concat([df_train[['question1', 'question2']], \\\n",
    "    df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "print ('making leaky features')\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "test_leaky = df_test.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "del df_test\n",
    "\n",
    "train_leaky = df_train.loc[:, ['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "\n",
    "# explore\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())\n",
    "\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist())\n",
    "\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "print('Building Features')\n",
    "X_train = build_features(df_train, stops, weights)\n",
    "print ('concatenating')\n",
    "X_train = pd.concat((X_train, X_train_ab, train_leaky), axis=1)\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Test Features\n",
      "reading test data\n",
      "going to function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/ipykernel_launcher.py:118: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/ipykernel_launcher.py:118: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenating\n"
     ]
    }
   ],
   "source": [
    "    # test processing\n",
    "    print('Building Test Features')\n",
    "    df_test = pd.read_csv('test_features.csv', encoding=\"ISO-8859-1\")\n",
    "    x_test_ab = df_test.iloc[:, 2:-1]\n",
    "    x_test_ab = x_test_ab.drop('euclidean_distance', axis=1)\n",
    "    x_test_ab = x_test_ab.drop('jaccard_distance', axis=1)\n",
    "    print ('reading test data')\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    df_test = df_test.fillna(' ')\n",
    "\n",
    "    df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n",
    "    df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())\n",
    "    print('going to function')\n",
    "    x_test = build_features(df_test, stops, weights)\n",
    "    print('concatenating')\n",
    "    x_test = pd.concat((x_test, x_test_ab, test_leaky), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv',index=False)\n",
    "x_test.to_csv('X_test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_features = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OWL'S Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.optimize import minimize\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "cvect = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "tfidf_txt = pd.Series(train['question1'].tolist() + train['question2'].tolist() + test['question1'].tolist() + test['question2'].tolist()).astype(str)\n",
    "tfidf.fit_transform(tfidf_txt)\n",
    "cvect.fit_transform(tfidf_txt)\n",
    "\n",
    "def diff_ratios(st1, st2):\n",
    "    seq = difflib.SequenceMatcher()\n",
    "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
    "    return seq.ratio()\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "    \n",
    "def get_features(df_features):\n",
    "    print('nouns...')\n",
    "    df_features['question1_nouns'] = df_features.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
    "    df_features['question2_nouns'] = df_features.question2.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
    "\n",
    "    print('tfidf...')\n",
    "    df_features['z_tfidf_sum1'] = df_features.question1.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_sum2'] = df_features.question2.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_mean1'] = df_features.question1.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_mean2'] = df_features.question2.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_len1'] = df_features.question1.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
    "    df_features['z_tfidf_len2'] = df_features.question2.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
    "    \n",
    "    print ('Caps features')\n",
    "    df_features['caps_count_q1'] = df_features['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df_features['caps_count_q2'] = df_features['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "    df_features['diff_caps'] = df_features['caps_count_q1'] - df_features['caps_count_q2']\n",
    "    \n",
    "    df_features['exactly_same'] = (df_features['question1'] == df_features['question2']).astype(int)\n",
    "    df_features['duplicated'] = df_features.duplicated(['question1','question2']).astype(int)\n",
    "    \n",
    "    print('some more')\n",
    "    add_word_count(df_features, df_features,'how')\n",
    "    add_word_count(df_features, df_features,'what')\n",
    "    add_word_count(df_features, df_features,'which')\n",
    "    add_word_count(df_features, df_features,'who')\n",
    "    add_word_count(df_features, df_features,'where')\n",
    "    add_word_count(df_features, df_features,'when')\n",
    "    add_word_count(df_features, df_features,'why')\n",
    "    return df_features.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns...\n",
      "tfidf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard wale features\n",
      "some more\n"
     ]
    }
   ],
   "source": [
    "trn_features = get_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns...\n",
      "tfidf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/alibaba/anaconda2/envs/py/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard wale features\n",
      "some more\n"
     ]
    }
   ],
   "source": [
    "tst_features = get_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_noun(df_features):\n",
    "    print('shared nouns')\n",
    "    df_features['z_noun_match'] = df_features.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)\n",
    "    return df_features.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared nouns\n",
      "shared nouns\n"
     ]
    }
   ],
   "source": [
    "trn_features = get_noun(trn_features)\n",
    "tst_features = get_noun(tst_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing unnecessary columns from train and test data\n",
    "trn_features = trn_features.ix[:,8:]\n",
    "tst_features = tst_features.ix[:,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290, 46), (2345796, 46))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404290, 79), (2345796, 79))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat((X_train, trn_features), axis=1)\n",
    "X_test = pd.concat((X_test, tst_features), axis=1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['word_match', 'tfidf_wm', 'tfidf_wm_stops', 'jaccard', 'wc_diff',\n",
       "        'wc_ratio', 'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unq_stop',\n",
       "        'wc_ratio_unique_stop', 'same_start', 'char_diff',\n",
       "        'char_diff_unq_stop', 'total_unique_words', 'total_unq_words_stop',\n",
       "        'char_ratio', 'len_q1', 'len_q2', 'diff_len', 'len_char_q1',\n",
       "        'len_char_q2', 'len_word_q1', 'len_word_q2', 'common_words',\n",
       "        'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
       "        'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
       "        'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd',\n",
       "        'cosine_distance', 'cityblock_distance', 'canberra_distance',\n",
       "        'minkowski_distance', 'braycurtis_distance', 'skew_q1vec',\n",
       "        'skew_q2vec', 'kur_q1vec', 'q1_q2_intersect', 'q1_freq', 'q2_freq',\n",
       "        'q1_pr', 'q2_pr', 'z_tfidf_sum1', 'z_tfidf_sum2', 'z_tfidf_mean1',\n",
       "        'z_tfidf_mean2', 'z_tfidf_len1', 'z_tfidf_len2', 'caps_count_q1',\n",
       "        'caps_count_q2', 'diff_caps', 'exactly_same', 'duplicated',\n",
       "        'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both',\n",
       "        'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who',\n",
       "        'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when',\n",
       "        'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both',\n",
       "        'z_noun_match'], dtype=object), (404290, 79))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 79)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 79 features are generated with page rank features which is generated in page_rank.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py]",
   "language": "python",
   "name": "conda-env-py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
