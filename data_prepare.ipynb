{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0c74f9c493e099d7c35d745791a13e35fe9e1e7bfcd9d70da72fbd6c2b1c73851",
   "display_name": "Python 3.6.13 64-bit ('tfenv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Quora Question Pair - Process Raw Data\n",
    "\n",
    " - created in 2021/5/9, Zhu Zhongbo, first trial with kaggle solutions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modules and global variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv,json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from os.path import expanduser, exists"
   ]
  },
  {
   "source": [
    " - codecs â€“ String encoding and decoding\n",
    "Purpose:\tEncoders and decoders for converting text between different representations.\n",
    "Available In:\t2.1 and later\n",
    "The codecs module provides stream and file interfaces for transcoding data in your program. It is most commonly used to work with Unicode text, but other encodings are also available for other purposes.\n",
    "\n",
    "- keras - api based on tensorflow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './data/'\n",
    "KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
    "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "Q1_TEST_DATA_FILE = 'q1_test.npy'\n",
    "Q2_TEST_DATA_FILE = 'q2_test.npy'\n",
    "TEST_ID_FILE = 'test_ids.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'"
   ]
  },
  {
   "source": [
    "## Data Prepare"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 404290 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['what is the step by step guide to invest in share market in india ', 'what is the story of kohinoor koh - i - noor diamond ', 'how can i increase the speed of my internet connection while using a vpn ', 'why am i mentally very lonely how can i solve it ', 'which one dissolve in water quikly sugar salt methane and carbon di oxide ', 'astrology : i am a capricorn sun cap moon and cap rising what does that say about me ', 'should i buy tiago ', 'how can i be a good geologist ', 'when do you use instead of ', 'motorola company : can i hack my charter motorolla dcx3400 ']\n"
     ]
    }
   ],
   "source": [
    "print(texts_1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['what is the step by step guide to invest in share market ', 'what would happen if the indian government stole the kohinoor koh - i - noor diamond back ', 'how can internet speed be increased by hacking through dns ', 'find the remainder when math 23 ^ 24 math is divided by 24 23 ', 'which fish would survive in salt water ', 'i am a triple capricorn sun moon and ascendant in capricorn what does this say about me ', 'what keeps childern active and far from phone and video games ', 'what should i do to be a great geologist ', 'when do you use instead of and ', 'how do i hack motorola dcx3400 for free internet ']\n"
     ]
    }
   ],
   "source": [
    "print(texts_2[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 2345796 texts in test.csv\n"
     ]
    }
   ],
   "source": [
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['how does the surface pro himself 4 compare with ipad pro ', 'should i have a hair transplant at age 24 how much would it cost ', 'what but is the best way to send money from china to the us ', 'which food not emulsifiers ', 'how aberystwyth start reading ', 'how are the two wheeler insurance from bharti axa insurance ', 'how can i reduce my belly fat through a diet ', 'by scrapping the 500 and 1000 rupee notes how is rbi planning to fight against issue black money ', 'what are the how best books of all time ', 'after 12th years old boy and i had sex with a 12 years old girl with her consent is there anything wrong ']\n"
     ]
    }
   ],
   "source": [
    "print(test_texts_1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(test_ids[0:10])"
   ]
  },
  {
   "source": [
    "## Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2, 3, 1, 1254, 61, 1254, 2921, 8, 578, 7, 759, 370, 7, 35], [2, 3, 1, 532, 10, 16563, 11914, 4, 22978, 4743], [5, 13, 4, 293, 1, 423, 10, 18, 334, 1724, 153, 127, 6, 2885], [15, 47, 4, 3396, 278, 3289, 5, 13, 4, 682, 17], [23, 48, 5750, 7, 204, 55553, 1593, 2208, 10718, 12, 1892, 7839, 5204], [2922, 4, 47, 6, 9205, 921, 4796, 825, 12, 4796, 5013, 2, 21, 30, 206, 50, 54], [29, 4, 122, 17729], [5, 13, 4, 28, 6, 42, 29375], [37, 9, 16, 71, 466, 10], [7208, 173, 13, 4, 549, 18, 13182, 98267, 93282]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences_1[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 120499 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 1), ('what', 2), ('is', 3), ('i', 4), ('how', 5), ('a', 6), ('in', 7), ('to', 8), ('do', 9), ('of', 10), ('are', 11), ('and', 12), ('can', 13), ('for', 14), ('why', 15), ('you', 16), ('it', 17), ('my', 18), ('best', 19), ('on', 20), ('does', 21), ('or', 22), ('which', 23), ('if', 24), ('have', 25), ('get', 26), ('with', 27), ('be', 28), ('should', 29), ('that', 30)]\n"
     ]
    }
   ],
   "source": [
    "print(list(word_index.items())[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of data tensor: (404290, 30)\nShape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n[[    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     2     3     1  1254    61  1254  2921     8\n    578     7   759   370     7    35]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     2     3     1   532\n     10 16563 11914     4 22978  4743]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     5    13     4   293     1   423    10    18\n    334  1724   153   127     6  2885]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0    15    47     4  3396   278\n   3289     5    13     4   682    17]\n [    0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0    23    48  5750     7   204 55553  1593\n   2208 10718    12  1892  7839  5204]]\n"
     ]
    }
   ],
   "source": [
    "print(type(data_1))\n",
    "print(data_1[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(2345796, 30)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_data_1[0:10]))\n",
    "print(test_data_1.shape)"
   ]
  },
  {
   "source": [
    "## GLOVE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "had the zip file, extract it\n",
      "Processing glove.840B.300d.txt\n",
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):\n",
    "    print(\"no such zip file, download it\")\n",
    "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
    "    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
    "elif exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE) and (not exists(KERAS_DATASETS_DIR + GLOVE_FILE)):\n",
    "    print(\"had the zip file, extract it\")\n",
    "    zipfile = ZipFile(KERAS_DATASETS_DIR+GLOVE_ZIP_FILE)\n",
    "    zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
    "\n",
    "print(\"Processing\", GLOVE_FILE)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null word embeddings: 33233\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of question1 data tensor: (404290, 30)\nShape of question2 data tensor: (404290, 30)\nShape of label tensor: (404290,)\nShape of question1 test data tensor: (2345796, 30)\nShape of question2 test data tensor: (2345796, 30)\nShape of label tensor: (2345796,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of question1 data tensor:', data_1.shape)\n",
    "print('Shape of question2 data tensor:', data_2.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Shape of question1 test data tensor:', test_data_1.shape)\n",
    "print('Shape of question2 test data tensor:', test_data_2.shape)\n",
    "print('Shape of label tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), data_1)\n",
    "np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), data_2)\n",
    "np.save(open(Q1_TEST_DATA_FILE, 'wb'), test_data_1)\n",
    "np.save(open(Q2_TEST_DATA_FILE, 'wb'), test_data_2)\n",
    "np.save(open(TEST_ID_FILE, 'wb'), test_ids)\n",
    "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
    "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
    "with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
    "    json.dump({'nb_words': nb_words}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}